/**
Instead of having to interactively go through each menu item and selecting the <Help> button to see what it’s about, a much simpler way to gain the same help information is to peek inside the relevant Kconfig* file (that describes the menu). Here, it’s lib/Kconfig.debug, as all (well, most) debug-related menus are there.
*/

cscope
ASAN, helgrind (from valgrind suite), TSan - userspace
KASAN for kernel
KCSAN - kernel concurrency sanitizer (conflicts with KASAN)
lockdep

// print any thread in 'D' (TASK_UNINTERRUPTIBLE) state,
// which may be a deadlock if stays for a long time
ps -LA -o state,pid,cmd | grep "^D"

tracing: perf-tools, ftrace (trace-cmd, KernelShark (gui)), LTTng, TraceCompass (gui)

%zu to printk size_t (32 and 64 bit portable!)
pr_debug() only when DEBUG symbol is defined
module_param_cb() - monitor param change
<kernel-src>/scripts/checkpatch.pl --no-tree -f <filename>.c

generate full Linux kernel documentation: 'make pdfdocs' in the root of the kernel source tree

https://github.com/agelastic/eudyptula

'crash' utility - investigate live system

--- check capabilities (root) ---
if (!capable(CAP_SYS_MODULE))
---

---
// access 'current' macro
#include <linux/sched.h>
current->pid, current-comm
---

---
// determine context of the thread (proc or interrupt)
// true if process, false if inside an interrupt
// Usage of 'current' is only valid in proc context
#include <linux/preempt.h>
if (in_task() && !in_atomic())
	// safe to sleep
// interrupt is always atomic!
---

---
// Memory (de)allocation
// GFP flags: GFP_KERNEL - normal, GFP_ATOMIC - atomic
(void *) __get_free_page(GFP_KERNEL)
(void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO), alloc_order)
(void *) get_zeroed_page(GFP_KERNEL)
(void *) get_zeroed_page(GFP_KERNEL)
page_address(alloc_page(GFP_KERNEL))
---
--- SLUB --
#include <linux/slab.h>
void *kmalloc(size_t size, gfp_t flags);
void *kzalloc(size_t size, gfp_t flags); // zeroed out memory
kmalloc_array(count, size, GFP_KERNEL) // instead of kmalloc(count * size, GFP_KERNEL)
and kzalloc() can be replaced with kcalloc()
void kfree(const void *);
kfree_sensitive(); // to zero out freed memory
// use to watch for overflow in first parameter
size_t size_mul()
size_t size_add()
size_t size_sub()
https://www.kernel.org/doc/html/latest/process/deprecated.html#deprecated-interfaces-language-features-attributes-and-conventions

// check actual number of bytes of memory allocated
size_t ksize(const void *);
---
// custom memset, compiler will not optimize it away
void *guaranteed_memset(void *v, int c, size_t n) {
	volatile char *p=v;
	while (n--)
		*p++ = c;
	return v;
}
memzero_explicit() in the kernel
---

helper methods to get task_struct info are in sched.h

RCU: https://reberhardt.com/blog/2020/11/18/my-first-kernel-module.html

---
// device memory alloc, called only in init() / probe()
// deallocation is automatic
// works only with GPL modules!
#include <linux/device.h>
void *devm_kmalloc(struct device *dev, size_t size, gfp_t gfp);
void *devm_kzalloc(struct device *dev, size_t size, gfp_t gfp);”
---

--- slab caches ---
#include <linux/slab.h>
kmem_cache_create()
kmem_cache_alloc()
kmem_cache_free()
kmem_cache_destroy

// if memory pressure gets high and OOM killer arrives,
// we can shrink some cache
register_shrinker()

// check memory wastage
grep . /sys/kernel/slab/our_ctx/* | grep -E "object_size|slab_size
---

-- vmalloc ---
// use when need >4MB of memory
// no guarantee of physically contiguous, but allocated right away!
// DON'T call in atomic context or while holding spinlock!
#include <linux/vmalloc.h>
void *vmalloc(unsigned long size);
void *vzalloc(unsigned long size);
void vfree(const void *addr);

// if unsure what to use, kmalloc() or vmalloc()
#include <linux/mm.h>
void *kvmalloc(size_t size, gfp_t flags);
void *kvzalloc(size_t size, gfp_t flags);

// for arrays
void *kvcalloc(size_t n, size_t size, gfp_t flags);
//for numa
void *kvmalloc_node(size_t size, gfp_t flags, int node);

void kvfree(const void *addr);
---

// run gcc only on the first 2 CPU cores
$taskset 03 gcc userspc_cpuaffinity.c -o userspc_cpuaffinity -Wall -O3

--- --- LOCKING --- ---
// always take locks in the same order (and document it)!!!
// order of releasing doesn't really matter

-- MUTEX --
#include <linux/mutex.h>
struct mutex mymtx;

// static init
DEFINE_MUTEX(mymtx);
// dynamic init
mutex_init(mymtx);

void __sched mutex_lock(struct mutex *lock);
void __sched mutex_unlock(struct mutex *lock);

// interruptible sleep for a loser thread
// don't know where to use really...
int mutex_lock_interruptible()
---

--- SPIN LOCK ---
#include <linux/spinlock.h>
// static
DEFINE_SPINLOCK(lock);
// dynamic
spinlock_t lock;
spin_lock_init(&lock);

void spin_lock(spinlock_t *lock);
void spin_unlock(spinlock_t *lock);

// lock and disable IRQs on this CPU
// so hardware interrupt won't race with process context (spinlock)
// if working on the same critical section
void spin_lock_irq(spinlock_t *lock);
spin_unlock_irq(&slock);

// lock, disable IRQs and restore only allowed IRQs
// set with flags earlier (disable_irq())
unsigned long spin_lock_irqsave(spinlock_t *lock, unsigned long flags);
void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);

// if softirq races with process context code path
void spin_lock_bh(spinlock_t *lock);
spin_unlock_bh();
---

--- atomic int / bit manip ---
#include <linux/atomic.h>
#include <linux/bitops.h>

RMW - Read Modify Write
Careful: These atomic APIs are atomic with respect to the CPU
core they’re running upon, but not with respect to other cores.

https://www.kernel.org/doc/html/v6.1/core-api/wrappers/atomic_t.html
atomic_t/atomic64_t

https://docs.kernel.org/core-api/kernel-api.html?highlight=test_and_set_bit#c.set_bit
void set_bit(unsigned int nr, volatile unsigned long *p);
void clear_bit(unsigned int nr, volatile unsigned long *p)
etc...

// refcount
// https://www.kernel.org/doc/html/latest/driver-api/basics.html#reference-counting
// transparently works with 32 and 64 bit
// can hold [1 - INT_MAX-1]
static refcount_t ga = REFCOUNT_INIT(42);
refcount_inc(&ga);
refcount_add(INT_MAX, &ga); // bug

// searching a bitmask
include/linux/find.h
unsigned long find_first_bit(const unsigned long *addr, unsigned long size);
etc...
---

--- Reader-writer spinlock ---
// used when lots of reads and occasional writes
// for opposite - lots of writes and few reads use 'sequence lock'
// writer can starve, ADVISED NOT TO USE, WILL BE REMOVED FROM KERNEL
#include <linux/rwlock.h
rwlock_t mylist_lock;”
void read_lock(rwlock_t *lock);
void write_lock(rwlock_t *lock);
read_unlock(rwlock_t *lock);
write_unlock(rwlock_t *lock);

__attribute__ ((aligned (64)));
// or
alignas() in C11
// to avoid false sharing / cache ping-pong
---

---
// per CPU variables //
---

--- RCU (Read-Copy-Update) lock free ---
// replacement for Reader-writer locks
// when lots of readers and few writers
// use RCU when reads are >= 90% of workload
---

--- Memory barriers ---
// instructions before the barrier are guaranteed to happen before
// the ones that are after
// Usually used with DMA descriptors only (if mentioned in datasheet)
<asm/barrier.h>
rmb(): Inserts a read (or load) memory barrier into the instruction stream.
wmb(): Inserts a write (or store) memory barrier into the instruction stream.
mb(): A general memory barrier

// example
desc->word0 = address;
wmb();
desc->word1 = DESC_VALID;

volatile - compiler won't reorder reads/writes with respect to
            other volatile variables. Not atomic tho!
            Usually used with MMIO.
---
